---
layout: blog-post
title: "CMU Probabilistic Graphical Models 10-708 Spring 2019 materials."
subtitle: |
            This page is intended as a resource for those wishing to self-study the graduate-level course “Probabilistic Graphical Models”, taught by Eric Xing to MS/MSc and PhD students in machine learning at Carnegie Mellon University in the spring of 2019. It contains all publicly available course materials from the official course page, work I completed as part of my own self-study; as well as my own comments and materials for those who wish to similarly benefit.
date:   2021-06-08 03:00:00 +0100
katex: True
---

**At the time of publishing, some uploads are still pending, to be completed.**

The original source from which all publicly available course materials have been extracted is the following course page:

> <https://sailinglab.github.io/pgm-spring-2019/>

I have archived these course materials in my GitHub repo below:

> <https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019>

The above repo can be cloned in its entirety if one wishes to download all the materials linked from this page at once.

For those wishing to use the materials from this course for self-study, the utility of this particular spring 2019 iteration is that there is a complete set of publicly available recorded video lectures, and also a complete set of homework assignments.

In order to facilitate the use of the course materials for those self-studying, I have based this page on the original course page as faithfully as possible. I have endeavoured to keep my own comments to a minimum, at least in the first part of the page.

In the second part of the page under [*"Self-study support"*](#self-study-support), I include additional comments specific to my own experiences with self-studying this course. I also include additional materials such as my scanned notes, lecture summaries, homework assignment write-ups and also supplementary reference texts that I personally found useful.

*Disclosure: I am an affiliate with [Bookshop.org](https://uk.bookshop.org/). This page contains affiliate links, so if you purchase a reference textbook through one of these links, you will not pay anything in addition to the listed price, but I will receive a small commission. If you wish to find out more about Bookshop, please visit their webpage.*

* Why should I study this course?
* Course description.
* Textbooks.
* Course grading.
* Course schedule.
* Homework Assignments.
* Projects.
* Subsequent iterations of this course.
* Self study support.
  * Additional comments on course prerequisites.
  * Additional comments on video lectures.
  * Additional comments on course content.
  * Additional comments on homework assignments.
  * Additional comments on reference textbooks.
  * Lecture summaries, session, notes, review notes.
  * Homework assignments: my write-ups.
  * Supplementary texts.
  * Electronic copies of reference textbooks.

## **Why should I study this course?**

As someone undertaking a programme of self-study in machine learning using CMU publicly available materials, this course was one of the more technically challenging, but also more rewarding courses.

One might think from the title of the course that it solely concerns methods related to an esoteric subfield of machine learning, i.e. probabilistic graphical models, like I did prior to studying it. However, the educational value of this course extends far beyond the seemingly limited realm of graphical models per se.

Firstly, on the relevance of graphical models for machine learning, the following is a quotation by Michael Jordan, from [this page on Kevin Murphy's website](https://www.cs.ubc.ca/~murphyk/Bayes/bnintro.html), both of whom are authors of textbooks used on the course:

> Graphical models are a marriage between probability theory and graph theory. They provide a natural tool for dealing with two problems that occur throughout applied mathematics and engineering -- uncertainty and complexity -- and in particular they are playing an increasingly important role in the design and analysis of machine learning algorithms. Fundamental to the idea of a graphical model is the notion of modularity -- a complex system is built by combining simpler parts. Probability theory provides the glue whereby the parts are combined, ensuring that the system as a whole is consistent, and providing ways to interface models to data. The graph theoretic side of graphical models provides both an intuitively appealing interface by which humans can model highly-interacting sets of variables as well as a data structure that lends itself naturally to the design of efficient general-purpose algorithms.
> 
> Many of the classical multivariate probabilistic systems studied in fields such as statistics, systems engineering, information theory, pattern recognition and statistical mechanics are special cases of the general graphical model formalism -- examples include mixture models, factor analysis, hidden Markov models, Kalman filters and Ising models. The graphical model framework provides a way to view all of these systems as instances of a common underlying formalism. This view has many advantages -- in particular, specialized techniques that have been developed in one field can be transferred between research communities and exploited more widely. Moreover, the graphical model formalism provides a natural framework for the design of new systems.
> 
> \- Michael Jordan (1998).

This course *will definitely* get one up to scratch on the foundational topics in graphical models, but some elaboration is required on how it goes beyond that.

In particular, having studied this course, I can attest to the above quotation; that graphical models provide an elegant underlying framework from which to better understand not only the  history of existing methods in probabilistic machine learning, but also how this history informs contemporary approaches e.g. deep learning.

Furthermore, I found that the selection of content on this course has supplied me with both the introductory literacy and consequently, the confidence, to be able to attempt to read machine learning research papers using more modern techniques not covered in introductory machine learning courses. Examples include *variational inference*, *Markov chain Monte Carlo methods* and their respective extensions; and also *Bayesian nonparametrics* and *deep generative models*. 

## **Course description.**

> Many of the problems in artificial intelligence, statistics, computer systems, computer vision, natural language processing, and computational biology, among many other fields, can be viewed as the search for a coherent global conclusion from local information. The probabilistic graphical models framework provides an unified view for this wide range of problems, enables efficient inference, decision-making and learning in problems with a very large number of attributes and huge datasets. This graduate-level course will provide you with a strong foundation for both applying graphical models to complex problems and for addressing core research topics in graphical models.

## **Textbooks.**

> The required textbook for this class is (note that the material of the class goes beyond this book):

[Koller, D., Friedman, N. (2009) *Probabilistic Graphical Models: Principles and Techniques.* MIT Press.](https://uk.bookshop.org/a/7712/9780262013192)

<p align="center">
  <a href="https://uk.bookshop.org/a/7712/9780262013192">
    <img src="/assets/2021-06-08-self-study-10-708/PGMPT2.jpg">
  </a>
</p>

> We will also be using excerpts from the following work, which you do not need to purchase:

Jordan, M. (draft). *An Introduction to Probabilistic Graphical Models.*

> Optional:

[Murphy, K. (2012). *Machine Learning: A Probabilistic Perspective.* MIT Press.](https://uk.bookshop.org/a/7712/9780262018029)

<p align="center">
  <a href="https://uk.bookshop.org/a/7712/9780262018029">
    <img src="/assets/2021-06-08-self-study-10-708/MLPP.jpg">
  </a>
</p>

Chapters from Michael Jordan's draft textbook are freely available at his homepage, and individual chapters prescribed to accompany each lecture are in course schedule below. All the chapters of his textbook can also be found and downloaded in their entirety [in this subdirectory](https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/tree/master/readings/intro-to-pgms-book-jordan) of my GitHub repo.

## **Course grading.**

> The class requirements include brief reading summaries, scribe notes for 1 lecture, 4 problem sets, and a project. This is a PhD level course, and by the end of this class you should have a good understanding of the basic methodologies in probabilistic graphical models, and be able to use them to solve real problems of modest complexity. The grading breakdown is as follows:
> 
> * Participation (4%)
> * Scribe Duties (10%)
> * Homework Assignments (40%)
> * Final Project (46%)
>
> Note that this class does not have any tests or exams.

## **Course schedule.**

The course is a 15 week course, including a one-week break. The lecture schedule was as follows.

|   Date.	|   Lectures.	|   Readings.	|
|---	|---	|---	|
|   1/14	|   Lecture #1 (Eric): <br> **Introduction to GM.** <br> [[slides][L1a] ([annotated][L1b]) \| [video][L1c] \| [notes][L1d]]	|   - Jordan, M. I. (2004). [Graphical models][L1r1]. <br>- Airoldi, E. (2007). [Getting started in probabilistic graphical models][L1r2]. <br> 	|
|   1/16	|   Lecture #2 (Eric): <br> **Representation: Directed GMs (BNs).** <br> [[slides][L2a] ([annotated][L2b]) \| [video][L2c] \| [notes][L2d]]	|   - Jordan textbook, [Ch. 2][L2r1]. (Sec. 2.1). <br> - Koller and Friedman textbook, [Ch. 3][L2r2]. <br>	|
|   1/21	|   No lectures (MLK day).	|   	|
|   1/23	|   Lecture #3 (Eric):<br> **Representation: Undirected GMs (MRFs).**<br> [[slides][L3a] ([annotated][L3b]) \| [video][L3c] \| [notes][L3d]]	|   - Jordan textbook, [Ch. 2][L3r1]. (Sec. 2.2 - end). <br>- Koller and Friedman textbook, [Ch. 4][L3r2]. <br> - Fischer, A. & Igel, C. (2012). [An introduction to restricted Boltzmann machines][L3r3]. <br>	|
|   1/28 <br> HW1 out.	|   Lecture #4 (Eric): <br> **Exact inference.** <br>- Elimination. <br>- Message passing.<br>- Sum product algorithm. <br> [[slides][L4a] ([annotated][L4b]) \| [video][L4c] \| [notes][L4d]]	|   - Jordan textbook, [Ch. 3][L4r1] and [Ch. 4][L4r2] <br>- Koller and Friedman textbook, [Ch. 9][L4r3], [Ch. 10][L4r4]. <br>- Minka, T. (2005). [Divergence measures and message passing][L4r5]. <br>	|
|   1/30	|   Lecture #5 (skipped): <br> **Parameter learning in fully observable Bayesian Networks.** <br> - Generalized Linear Models (GLIMs) <br>- Maximum Likelihood Estimation (MLE) <br>- Markov Models. <br> [[slides][L5a] \| [notes][L5d]]	|   - Jordan textbook, [Ch. 8][L5r1], [Ch. 9][L5r2] (Sec. 9.1 - 9.2). <br> - Koller and Friedman textbook, [Ch. 17][L5r3] (Sections 17.1 - 17.4). <br> - Geiger, D. & Heckerman, D. (2002). [Parameter priors for directed acyclic graphical models and the characterization of several probability distributions][L5r4]. <br> - Geiger, D. & Heckerman, D. (1997). [A characterization of the Dirichlet distribution through global and local parameter independence][L5r5].<br>	|
|   2/4	|   Lecture #6 (Maruan): <br> **Parameter Learning of partially observed BNs.** <br>- Mixture models.  <br>- Hidden Markov Models (HMMs). <br>- The EM algorithm. <br> [[slides][L6a] ([annotated][L6b]) \| [video][L6c] \| [notes][L6d]]	|   - Jordan textbook, [Ch. 11][L6r1]. <br>- Koller and Friedman textbook, [Ch. 19.1-19.4.][L6r2] <br>- Borman, S. (2006). [The EM algorithm (A short tutorial)][L6r3]. <br>- Neal, R. & Hinton, G. (1998). [Some interesting aspects of the EM algorithm][L6r4]. <br>	|
|   2/6	|   Lecture #7 (Eric): <br> **Maximum likelihood learning of undirected GM.** <br> [[slides][L7a] ([annotated][L7b]) \| [video][L7c] \| [notes][L7d]]	|   - Jordan textbook, [Ch. 9][L7r1] (Sec. 9.3-9.5), [Ch. 20][L7r2]. <br>- Friedman, J. et al. (2008). [Sparse inverse covariance estimation with the graphical lasso][L7r3]. <br>- Lafferty, J. et al. (2001). [Conditional random fields - Probabilistic models for segmenting and labeling sequence data][L7r4]. <br>	|
|   2/11	|   Lecture #8 (guest lecture, Kun Zhang @ Department of Philosophy): <br> **Causal discovery and inference.** <br> [[slides][L8a] \| [video][L8c] \| [notes][L8d]]|   - Pearl, J. et al. (2016). [Causal Inference in Statistics: A Primer][L8r1]. Concepts related to causality and identification of causal effects.  <br>- Sprites, P. et al. (2001). [Causation, Prediction, and Search][L8r2]. Pages 80-89 and 123-136. Traditional methods for causal discovery, including the PC and FCI algorithms. <br>- Zhang, K. et al. (2017). [Learning causality and causality-related learning.][L8r3] Brief review of recent methods for causal discovery. <br>	|
|   2/13 <br> HW1 due.	|   Lecture #9 (Eric): <br> **Modeling networks.** <br>- Gaussian graphical models. <br>- Ising models. <br> [[slides][L9a] ([annotated][L9b]) \| [video][L9c] \| [notes][L9d]]	|   - Meinshausen, N. & Bühlmann, P. (2006). [High-dimensional graphs and variable selection with the Lasso][L9r1]. <br>- Kolar, M. et al. (2010). [Estimating time-varying networks][L9r2]. <br>- Dempster, A. (1972). [Covariance selection][L9r3]. <br>	|
|   2/18	|   Lecture #10 (Eric): <br> **Sequential models.** <br>- Discrete Hidden State (HMM vs. CRF). <br> - Continuous Hidden State (Kalman Filter). <br> [[slides][L10a] ([annotated][L10b]) \| [video][L10c] \| [notes][L10d]]	|   - Jordan textbook, [Ch. 14][L10r1], [Ch. 15][L10r2]. <br>- Ng, A. [Lecture notes on factor analysis][L10r3]. <br>- Welch, G. & Bishop, G. (2006). [An introduction to the Kalman filter][L10r4]. <br>- Wallach, H. (2004). [Conditional random fields - An introduction][L10r5]. <br>	|
|   2/20 <br> HW2 out.	|   Lecture #11 (Eric): <br> **Approximate Inference: Mean Field (MF) and loopy Belief Propagation (BP) approximations.** <br> [[slides][L11a] ([annotated][L11b]) \| [video][L11c] \| [notes][L11d]]	|   - Yedidia, J. et al. (2000). [Generalized belief propagation][L11r1]. <br>- Xing, E. et al. (2003). [A generalized mean field algorithm for variational inference in exponential families][L11r2]. <br>- Mohamed, S. et al. (2016). [Variational inference tutorial (NIPS 2016)][L11r3]. <br>	|
|   2/25	|   Lecture #12 (Eric): <br> **Theory of Variational Inference: Inner and Outer Approximations.** <br> [[slides][L12a] ([annotated][L12b]) \| [video][L12c] \| [notes][L12d]]	|   - Wainwright, M. & Jordan, M. (2003). [Variational inference in graphical models - The view from the marginal polytope][L12r1]. <br>- Wainwright, M. & Jordan, M. (2008). [Graphical models, exponential families, and variational inference][L12r2] (Sec. 3 and 4). <br>	|
|   2/27	|   Lecture #13 (Eric): <br> **Approximate Inference: Monte Carlo and Sequential Monte Carlo methods.** <br> [[slides][L13a] ([annotated][L13b]) \| [video][L13c] \| [notes][L13d]]	|   - Jordan textbook, [Ch. 21][L13r1]. <br>- Mackay, D. (2003). *Information Theory, Inference and Learning Algorithms*, [Ch. 29][L13r2] (Sec. 29.1-29.3). <br>	|
|   3/4	|   Lecture #14 (Eric): <br> **Markov Chain Monte Carlo.** <br>- Metropolis-Hastings. <br>- Hamiltonian Monte Carlo. <br>- Langevin Dynamics. <br> [[slides][L14a] ([annotated][L14b]) \| [video][L14c] \| [notes][L14d]]	|   - Mackay, D. (2003). *Information Theory, Inference and Learning Algorithms*, [Ch. 29][L14r1] (Sec. 29.4-29.10). <br>- Neal, R. (2011). [MCMC using Hamiltonian dynamics][L14r2]. <br>- Patterson, S. & Teh, Y. (2013). [Stochastic gradient Riemannian Langevin dynamics on the probability simplex][L14r3]. <br>	|
|   3/6 <br> HW2 due.	|   Lecture #15 (Eric): <br> **Statistical and Algorithmic Foundations of Deep Learning.** <br>- Insight into DL. <br>- Connections to GM. <br> [[slides][L15a] ([annotated][L15b]) \| [video][L15c] \| [notes][L15d]]	|   - Goodfellow, I. et al. (2016). *Deep Learning*, [Ch. 6.2-5][L15r1], [20.3-4][L15r2]. <br>- Salakhutdinov, R. & Hinton, G. (2009). [Deep Boltzmann machines][L15r3]. <br>- Belanger, D. & McCallum, A. (2016). [Structured prediction energy networks][L15r4]. <br>- Ranganath, R. et al. (2015). [Deep exponential families][L15r5]. <br>	|
|   3/11	|   No class (Spring Break).	|   	|
|   3/13	|   No class (Spring Break).	|   	|
|   3/18 <br> HW3 out.	|   Lecture #16 (guest lecture, Zhiting Hu): <br>**Building blocks of DL.** <br>- RNN and LSTM. <br>- CNN, Transformers. <br>- Attention mechanisms. <br>- (Case studies in NLP). <br> [[slides][L16a] \| [video][L16c] \| [notes][L16d]]	|   - Pascanu, R. et al. (2013). [On the difficulty of training recurrent neural networks][L16r1]. <br>- Vaswani, A.  et al. (2017). [Attention is all you need][L16r2]. <br>- Devlin et al. (2019). [BERT - Pre-training of deep bidirectional transformers for language understanding][L16r3]. <br>	|
|   3/20	|   Lecture #17 (Eric): <br> **Deep generative models (part 1)**\: <br>**Overview of the theoretical basis and connections of deep generative models.**<br>- Wake sleep algorithm. <br>- Variational autoencoders. <br>- Generative adversarial networks. <br>- A unified view of DGM. <br> [[slides][L17a] \| [video][L17c] \| [notes][L17d]]	|   - Goodfellow, I. et al. (2016). *Deep Learning*, [Ch. 20.9-10][L17r1]. <br>- Kingma, D. & Welling, M. (2014). [Auto-encoding variational Bayes.][L17r2]. <br>- Goodfellow, I. et al. (2014). [Generative adversarial nets][L17r3].  <br>- Sanjeev, A. [GANs, some open questions (blog)][L17r4]. <br>	|
|   3/25	|   Lecture #18 (guest lecture, Zhiting Hu): <br> **Deep generative models (part 2).** <br>- GANs and their variations. <br>- Normalizing Flows. <br>- Integrating domain knowledge in DL. <br> [[slides][L18a] \| [video][L18c] \| [notes][L18d]]	|   - Arjovsky, M. & Bottou, L. (2017). [Towards principled methods for training generative adversarial networks][L18r1]. <br>- Kingma, D. & Dhariwal, P. (2018). [Glow - Generative flow with invertible 1x1 convolutions][L18r2]. <br>- Hu, Z. et al. (2016). [Harnessing deep neural Networks with logic rules][L18r3]. <br>- Hu, Z. et al. (2018). [Deep generative models with learnable knowledge constraints][L18r4]. <br>	|
|   3/27 <br> Midway report due.	|   Lecture #19 (guest lecture, Zhiting Hu): <br>**Case Study: Text Generation.** <br>- The encoder-decoder framework <br>- Machine translation as conditional generation. <br>- Unifying MLE and RL objectives for text generation. <br> [[slides][L19a] \| [video][L19c] \| [notes][L19d]]	|   - Ranzato, M. et al. (2016). [Sequence level training with recurrent neural networks][L19r1]. <br>- Hu, Z. et al. (2017). [Toward controlled generation of text][L19r2]. <br>- Tan, B. et al. (2019). [Connecting the dots Between MLE and RL for sequence generation][L19r3]. <br>	|
|   4/1	|   	Lecture #20 (Maruan): <br>**Sequential decision making (part 1): The framework.** <br>- Brief introduction to reinforcement learning (RL). <br>- Connections to GM: RL and control as inference. <br>- Control via Variational Inference. <br> [[slides][L20a] ([annotated][L20b]) \| [video][L20c] \| [notes][L20d]]	|  - Sutton, R. & Barto, A. (2018). *Reinforcement Learning: An Introduction*, [Ch. 3][L20r1], [Ch. 4][L20r2]. <br>- Lilian Weng. [A peek into RL (blog)][L20r3]. <br>- Levine, S. (2018). [Reinforcement learning and control as probabilistic inference][L20r4], Sec. 1-3. <br>- Ziebart, B. (2010). [Modeling purposeful adaptive behavior with the principle of maximum causal entropy][L20r5], Ch. 5.1-2, 6.1-2. <br>- Todorov, E. (2008). [General duality between optimal control and estimation][L20r6]. <br>- Koller and Friedman textbook, [Ch. 20.3][L20r7]. <br>	|
|   4/3 <br> HW3 due.	|   Lecture #21 (Maruan): <br>**Sequential decision making (part 2): The algorithms.** <br>- Intro to RL algorithms: policy gradients and Q-learning. <br>- Max-entropy policy gradient. <br>- Soft Q-learning. <br> [[slides][L21a] ([annotated][L21b]) \| [video][L21c] \| [notes][L21d]]	|   - Sutton, R. & Barto, A. (2018). *Reinforcement Learning: An Introduction*, [Ch. 13][L21r1]. <br>- Levine, S. (2018). [Reinforcement learning and control as probabilistic inference][L21r2], Sec. 4. <br>- Haarnoja, T. et al. (2017). [Reinforcement learning with deep energy-based policies][L21r3]. <br>- Haarnoja, T. et al. (2018). [Soft actor-critic][L21r4]. <br>	|
|   4/8 <br> HW4 out.	|   Lecture #22 (Eric): <br> **Bayesian nonparametrics.** <br>- Dirichlet Process (DP). <br>- Indian Buffet Process (IBP). <br> [[slides][L22a] ([annotated][L22b]) \| [video][L22c] \| [notes][L22d]]	|   - Teh, Y. (2011). [The Dirichlet process][L22r1]. <br>- Griffiths, T. & Ghahramani, Z. (2011). [The Indian buffet process][L22r2]. <br>	|
|   4/10	|   Lecture #23 (Maruan): <br>**Bayesian non-parametrics (continued).** <br>- Inference in Dirichlet Process (DP). <br>- Hierarchical Dirichlet Process (HDP). <br>- Indian Buffet Process (IBP). <br> [[slides][L23a] \| [video][L23c] \| [notes][L23d]]	|   - Hopper, T. [Notes on Dirichlet processes (webpage)][L23r1]. <br>- Ishwaran, H. & James, L. (2001). [Gibbs sampling methods for stick-breaking priors][L23r2]. <br>- Teh, Y. et al. (2006). [Hierarchical Dirichlet process][L23r3]. <br>	|
|   4/15	|   Lecture #24 (Eric): <br>**Integrative Paradigms of GM: Regularized Bayesian Methods.** <br> [[slides][L24a] ([annotated][L24b]) \| [video][L24c] \| [notes][L24d]]	|   - Ganchev, K. et al. (2010).  [Posterior regularization for structured latent variable models][L24r1]. <br>- Zhu et al. (2014). [Bayesian inference with posterior regularization and applications to infinite latent SVMs][L24r2]. <br>	|
|   4/17	|   Lecture #25 (Eric): <br> **Elements of Spectral & Kernel GMs.** <br> [[slides][L25a] ([annotated][L25b]) \| [video][L25c] \| [notes][L25d]] |  - Song, L. (2008). [Learning via Hilbert space embedding of distributions][L25r1], Sec. 2.1, 2.2, 3.1, 3.2. <br>- Hsu, D. et al. (2012). [A spectral algorithm for learning hidden Markov models][L25r2]. <br>- Song, L. et al. (2012). [Nonparametric latent tree graphical models][L25r3]. <br>- Parikh, A et al. (2011).   [A spectral algorithm for latent tree graphical models][L25r4]. <br>	|
|   4/22	|   Lecture #26 (Maruan): <br>**Gaussian processes (GPs) and elements of meta-learning.** <br>- Gaussian Processes (GPs) and kernel functions. <br>- (Deep) kernel learning and approximations. <br>- Neural Processes (NPs) as an approximation to GPs. <br>- Elements of meta-learning. <br> [[slides][L26a] \| [video][L26c] \| [notes][L26d]]	|   - Rasmussen, C. & Williams, C. (2003). *Gaussian Processes for Machine Learning*, [Ch. 2.2-2.4][L26r1], [Ch. 4.1-4.2][L26r2]. <br>- Görtler, J. et al. (2019). [A visual exploration of Gaussian processes (blog)][L26r3]. <br>- Wilson et al. (2015). [Deep kernel learning][L26r4]. <br>- Garnelo, M. et al. (2018). [Neural processes][L26r5]. <br>	|
|   4/24 <br> HW4 due.	|   Lecture #27 (guest lecture, Qirong Ho): <br> **Scalable algorithms and systems for learning, inference, and prediction.** <br> [[slides][L27a] \| [video][L27c] \| [notes][L27d]]	|   - Bradley, J. et al. (2011). [Parallel coordinate descent for L1-regularized loss minimization][L27r1]. <br>- Yuan, J. et al. (2015). [LightLDA - Big topic models modest computer clusters][L27r2]. <br>- Ho, Q. et al. (2013). [More effective distributed ML via a stale synchronous parallel parameter server][L27r3]. <br>- Kim, J. et al. (2016). [STRADS - A distributed framework for scheduled model parallel machine learning][L27r4]. <br>	|
|   4/29	|   Lecture #28 (Eric): <br> **A civil engineering perspective on AI.** <br> [[slides][L28a] ([annotated][L28b]) \| [video][L28c] \| [notes][L28d]]	|   - Xing, E. et al. (2015). [Petuum - A new platform for distributed machine learning on Big Data][L28r1]. <br>- Xing, E. et al. (2016). [Strategies and principles of distributed machine learning on Big Data][L28r2]. <br>|
|   4/30	|   Project presentations.	|   	|

A complete set of lecture videos were uploaded to YouTube, and are individually linked above, except for lecture 5, "*Parameter learning in fully observable Bayesian networks*", which was skipped in the course itself.

These recorded lectures contain instructor-led exposition of the lecture slides, on which annotations are made.

Lecture slides for all sessions are individually linked above, but can be downloaded in their entirety from [this subdirectory][GR1] of my GitHub repo.

Instructor annotated lecture slides are also individually linked above, but not every lecture has a set of annotated slides. All annotated lecture slides can be downloaded in their entirety from [this subdirectory][GR2].

All prescribed readings are individually linked, as are notes in the form of distill.pub style blog articles scribed by CMU students taking the course. All the prescribed readings can be downloaded in their entirety from [this subdirectory][GR3].

I have also included my own additional materials that I developed whilst watching these lectures. These additional materials can be found further down in the 2nd part of this page under [*“Lecture summaries, session notes, review notes.”*](#lecture-summaries-session-notes-review-notes)

## **Homework Assignments.**

> There will be 4 homework assignments over the course of the semester. These assignments may contain material that has been covered by published papers and webpages. It is a graduate class and we expect students to solve the problems themselves rather than search for answers.
> 
> Students are required to typeset homework solutions using LaTeX and the provided template.
>
> Collaboration Policy
>
> Homework assignments must be done individually: each student must hand in their own answers. However, it is acceptable to collaborate when figuring out answers and to help each other solve the problems. We will be assuming that, as participants in a graduate course, you will be taking the responsibility to make sure you personally understand the solution arising from such collaboration. You also must indicate on each homework with whom you have collaborated.

Homework 1.
* [Assignment PDF handout][HW1a].
* [Assignment LaTeX template][HW1b].
* [Starter code][Hw1c].
* [Supplementary note on hidden Markov models][Hw1d].

Homework 2.
* [Assignment PDF handout][HW2a].
* [Assignment LaTeX template][HW2b].
* [Data sets][HW2c]. 

Homework 3.
* [Assignment PDF handout][HW3a].
* [Assignment LaTeX template][HW3b].
* [Colab notebook][HW3c].

Homework 4.
* [Assignment PDF handout][HW4a].
* [Assignment LaTeX template][HW4b].
* [Colab notebook][HW4c].

All homework assignment materials can be downloaded in their entirety in [this subdirectory][GR4] of my GitHub repo.

There are no official, publicly available course solutions for the assignments. For those self-studying, please see the section [*"Additional comments on homework assignments"*](#additional-comments-on-homework-assignments) in the 2nd part of this page.

## **Projects.**

I have located this part of the course page in another location at:

*insert link, pending construction.*

## **Subsequent iterations of this course.**

At the time of writing, there is an additional spring 2020 iteration of the course, also taught by Eric Xing, and the coursepage is below:

> <https://www.cs.cmu.edu/~epxing/Class/10708-20/>

And there is also a later spring 2021 iteration of the course, taught by Matt Gormley, and the coursepage is below:

> <http://www.cs.cmu.edu/~mgormley/courses/10708/index.html>

On the spring 2020 iteration of this course, the content and the content archived on this page are *almost entirely identical*, with the minor exception of at most 2 later lecture slide sets. All lecture slides and videos are available for this iteration, and prescribed readings are the same, but it does not possess a complete set of homework assignments due to the disruption caused by COVID-19. I will endeavour to list some of these minor differences when I have had a chance to audit it more comprehensively. 

On the spring 2021 iteration by Matt Gormley, I cannot comment as I have only had a cursory glance at course page. However, it would seem that the lecture content has become more geared towards said instructor's research interests, inevitably.

## **Self-study support.**

In what follows, I include some additional comments from my own experiences self-studying the course. This may be helpful to those wishing to make use of this course in a similar fashion.

I also include additional materials that I wrote when self-studying this course. These include my own handwritten lecture notes and high-level summaries for each recorded video lecture; write-ups of my own attempts at the homework assignments with accompanying code for which solutions are not available; and supplementary reference textbooks that I found useful.

## **Additional comments on course prerequisites.**

The official course page does not list any course prerequisites.

However, at times, aspects of the course are uncompromising in terms of the level of the technical material covered and readings suggested, which is why it is stated under [*"Course grading"*](#course-grading) that this is a PhD level course (although I suspect on the 'easier' spectrum of PhD level).

Therefore, the course can be technically demanding at times. First, a little needs to be said on what these technical demands are *not*. The technical demands of the course do not encompass being fluent in any sophisticated formal mathematical machinery e.g. in measure theory, stochastic processes, functional analysis etc. which would require substantial prior training.

*Mathematical prerequisites.*

In respect of formal mathematical fluency, only calculus, linear algebra, probability and statistics are required. The following are some minimum requirement pointers:

* Linear algebra - "*computational*" fluency is required. That is, one should be fluent with elementary operations and manipulations with vectors and matrices, to the extent that one can read and reconstruct proofs of basic results and identities. Fluency with the *theory of linear algebra*, in the sense of the *study of linear maps on finite-dimensional vector spaces*, while desirable in general, is not strictly necessary for the course.
* Calculus - "*computational*" fluency is required. That is, one should be fluent with *computing derivatives for optimisation problems*, and in particular *multivariate generalisations* such as *gradients, Jacobians and Hessians.*
* Probability - basic fluency is required. Fluency with manipulation of basic probability statements via application of probability rules is absolutely essential both to the exact inference and approximate inference parts of the course.
* Statistics - basic fluency is required. In particular, familiarity in dealing with multivariate objects in a statistical setting e.g. computing expectations; and also familiarity with the notion of a statistic, maximum likelihood estimation. Familiarity with Bayesian statistics is *essential*.

*Prerequisites for homework assignment completion.*

Programming skills in [*Python*](https://www.python.org/) and also [*NumPy*](https://numpy.org/) are essential to be able to complete the four homework assignments, because some of these will require one to write implementations of algorithms from scratch without recourse to off-the-shelf packages.

*Background knowledge.*

With respect to keeping up with the pace of the lectures, and in being able to derive significant value from the suggested readings, most of the technical demands lie in the requirement for a previous acquaintance with standard machine learning methods covered in an introductory graduate machine learning course. That is because many of these methods *will be reinterpreted through the framework of probabilistic graphical models in the lectures*.

I therefore *strongly recommend* that those wishing to use this course self-study already have acquaintance with the following: *perceptrons, Bayesian linear regression, logistic regression, LASSO or $$l_1$$-regularised regression, Gaussian mixture models, k-means algorithm, parameter estimation and inference in hidden Markov models, expectation-maximisation (EM) algorithm, latent Dirichlet allocation (LDA), simple multi-layer perceptrons and the backpropagation algorithm.*

In my view, attempting to derive significant value from the graphical model reinterpretation of the above methods without existing underlying knowledge of these methods would be difficult. Attempting to take self-study detours to learn about the above methods in parallel with assimilating the already technical material of the lectures is not impossible, but in my view, is not advised. 

Lastly, I recommend spending some time reading "*Chapter 8: Graphical Models*" of *Pattern Recognition and Machine Learning* by Bishop (2006) to get a sense of the foundational topics the course covers, before undertaking self-study. The course however goes much further than this. As the chapter is around 80 pages, there is no need to read it entirely, rather dip into it. I have included specific suggestions on what to read from this chapter, as well as a link to it under [*"Supplementary texts."*](#supplementary-texts)

On my own background knowledge before undertaking self-study in this course, I had already completed a graduate-level machine learning course, and had already gone through most of the chapters of *Pattern Recognition and Machine Learning* by Bishop (2006) quite comprehensively.

## **Additional comments on video lectures.**

The exposition in the video lectures is on the whole very clear. The utility of watching these lectures over and above just reading the slides and doing the recommended readings cannot be overstated.

Viewing the content of the lecture slides before having watched the lecture recordings might make one believe that the material is intimidatingly technical. However, one is clearly guided through the more difficult derivations by the instructor Eric Xing, and other guest lecturers, and any derivations not covered are generally routine.

However, where I would place the greatest educational value would be in the motivations and bird's eye view supplied by the experienced instructors on the material covered. With Eric Xing's instruction in particular, it was emphasised to me that *techniques in and of themselves are not what counts, rather, what counts is the manner in which it is deployed to achieve a vision, or to solve a particular aspect of a problem.* The strong sense I took away from this is that there is a scientifically informed *artistry* in the selection of what machine learning techniques one wishes to use to solve or *flexibly adapt* to a particular aspect of a problem.

A consequence of this perspective is that for the more advanced topics, such as the use of *gradient information in Hamiltonian Monte Carlo methods* to address the inefficient *random walk behaviour* of *Markov chain Monte Carlo methods*; or the use of *stochastic approximation* in *stochastic variational inference*; these become, in hindsight, *natural* and almost *obvious* extensions to existing methods.

*For me, it is this viewpoint that was the singularly most valuable take away from the course, one which only comes from an experienced instructor, and which is difficult to holistically communicate in monographs or tutorials.*

On the technical side, there are *no issues* with audio discernibility nor with video clarity.

There is an alternative set of links for the video lectures, whose video quality is clearer, but not overly noticeably so, and these are hosted on a publicly available part of CMU Panopto here:

> <https://scs.hosted.panopto.com/Panopto/Pages/Sessions/List.aspx#folderID=%220f44b4d7-fb4e-49eb-b88d-a9d00125e1b3%22>

YouTube links instead of the CMU Panopto links were included as it was decided that the former might be more 'permanent' than the latter.

## **Additional comments on course content.**

The course seems to have been running at CMU for many years, and I suspect that its technical demands are by deliberate design. That is, the motivation is to supply MS and PhD students with literacy in state-of-the-art techniques regularly encountered in research, rather than presenting content which has been watered down for accessibility.

The course is divided into 5 mini-modules:

* Module 1: Introduction, representation and exact inference.
* Module 2: Approximate inference.
* Module 3: Deep learning and generative models.
* Module 4: Reinforcement learning and control through inference in graphical models.
* Module 5: Nonparametric methods.

Module 1 is an extremely fast-paced introduction to foundational topics in probabilistic graphical models, such as representation, parameter estimation, and exact inference. Some additional topics such as causal inference, network structure learning, and sequential models are also included.

Module 2 introduces both deterministic and stochastic approximate inference techniques. The former covers *mean-field variational inference*, and the *theory of variational inference*. The latter introduces *Monte Carlo*, *sequential Monte Carlo*, *Markov chain Monte Carlo* methods and further extensions.

Module 3 looks at the interplay between graphical models and deep learning, mainly through the recent development of *deep generative models*.

Module 4 and Module 5: tbc

## **Additional comments on homework assignments.**

These comprise a mix of theoretical questions involving mathematical proof or the derivation of an algorithm; and programming exercises where an algorithm needs to be implemented from scratch on a pre-processed data set.

For CMU students, these derivations would be marked by a teaching assistant, and the code implementation would be marked by a scripted automatic grading system, or a teaching assistant.

As part of an audit almost a year ago on whether the assignments could be completed as part of self-study, outside of the support structure of formal education, I compiled the following summary of what each of the four assignments consisted of.

* Homework assignment 1 - A mix of exercises and derivations taken from Koller and Friedman (2009). Programming element consists of implementing a junction tree algorithm and an EM algorithm for inference in a hidden Markov model. Script skeletons are provided and one has to fill in most of the code for submission to Gradescope.

* Homework assignment 2 - Derivations of algorithms for parameter estimation and inference in a hidden Markov model and in a linear-chain conditional random field; of algorithms for variational inference in a sequence-augmented latent Dirichlet allocation model, and of the Metropolis algorithm for posterior inference in a hierarchical Bayesian model. Programming element consists of implementing all derived algorithms from scratch in code on provided data sets for parts-of speech tagging (Penn Treebank corpus), topic modelling (Wikipedia dataset), and sports analytics (English Premier League data), without recourse to off-the-shelf packages. Results are expected as a PDF write-up, and algorithm implementations as Python scripts.

* Homework assignment 3 - A series of derivations and code implementations for various algorithms related to the variational autoencoder, and generative adversarial networks. Code is used create results that are written up. Results are submitted as a Jupyter/Colab notebook.

* Homework assignment 4 - Derivations and code implementations for 
a reinforcement learning problem; and some derivations on Bayesian nonparametrics. Results are submitted as Jupyter/Colab notebook.

For those who may wish to complete these assignments as part of self-study, it might seem a daunting prospect as to whether these technical assignments can be completed, especially without the support structure of a formal educational environment.

The following is some general advice from my own experience with the course homework assignments during self-study.

For simpler theoretical derivation exercises, these tend to be doable, and success here will depend on how well one has assimilated the contents of the lectures. Whilst the derivations at times concern technically demanding content, they are not so impenetrable so as not be made easier by further reading around the subject.

For the more involved derivations where one has to use a procedure to derive an algorithm, these are based on or adapted from existing tutorial papers. My advice here would be that one really needs to completely deconstruct the relevant aspects of the tutorial papers with surgical precision and intellectual honesty. That is, one needs to fully understand all aspects of the method being applied to derive the algorithm. In the case that one is struggling to understand, one needs the strength to be able to admit this. My advice in this case would be to seek further supplementary materials, such as other tutorial papers or reputable references. Because any deficiencies in understanding at the pre-algorithmic, mathematical derivation stage will be revealed or compounded during the code implementation, where one has to additionally worry about coding bugs.

For code implementations that would normally be marked by the automated grading system or teaching assistants, not available to those self-studying, one will have to aim for *algorithm correctness as far as possible within the parameters defined by the assignment.* However those self-studying are *not* working completely blind.

For the more elementary code implementations, there are often very concrete mathematical constraints or behaviours that the implementation needs to exhibit that can be used to ascertain whether one is getting correct results. There are also other ways to check one's code implementation, e.g. in parameter estimation, one can generate synthetic data with known parameters, and see if these known parameters are recovered.

Similar principles apply to the more involved algorithm implementations, there will be mathematical constraints or characteristic behaviours that the implementation will need to abide by or exhibit.

More generally, standard software engineering best practices apply. Write simple code to begin with, test it on simple examples, and slowly build up the complexity piece by piece. Doing so allows one to diagnose where bugs might be, or provide information to better guesstimate where the bugs are. Debugging code is often made easier when one has security that the modular pieces are working correctly individually.

The special nature of self-study outside of a formal educational environment allows for some leeway with the use of off-the-shelf packages. Whilst the objectives of the coding assignments are to teach one a technique by implementing it from scratch, and not use an off-the-shelf package, the lack of concrete, specific assistance from teaching assistants and peers means that if one is truly stuck a source of assistance might be to learn to use an off-the-shelf package on the data set, but only as a gold standard to measure one's own implementation against. This would also mean developing literacy in that package as an added bonus.

On my own experiences with the homework assignments, some of them took much longer than I anticipated, and there was a great deal of 'feeling around in the dark', which at points became demoralising. Meaning that I had to spend a lot of time deconstructing the relevant papers, or searching for other references or tutorials when I wasn't able to resolve a particular point. On most occasions, I was able to make it through.

For this reason, I have compiled a specific list of hints and tips; as well as additional references I found useful whilst doing the assignments.

*insert PDF.*

If one chooses to do these assignments in this way, then the added bonus is that one will develop skills to be able to reproduce a machine learning paper in code from scratch. While this will not be production-ready, it does induce confidence that one has understood a method enough to be able to engineer it from the ground up. Because ultimately, pre-production algorithms are instantiations of mathematical ideas.

## **Additional comments on reference textbooks.**

Throughout the course, I consulted all three recommended course textbooks equally frequently. Here are some comments on each textbook, and how I used them while self-studying the course.

I found that the draft textbook *Introduction to Probabilistic Graphical Models* by Jordan to excel in its clarity of exposition, and I relied on this textbook most heavily when it came to developing a clear understanding of the foundational topics in graphical models, including but not limited to, representation; graphical model interpretations of sequential models; parameter estimation in fully and partially observed graphical models; and exact inference procedures such as message-passing and the junction tree algorithm.

Generally, I found that if one of the topics on the course overlapped with any of book chapters, this book would be the first place I would start. That this book is so remarkably clear is no surprise, Michael Jordan was the mentor of the course instructor, Eric Xing, and also had strong associations with the course textbook authors, Kevin Murphy, and Daphne Koller.

The textbook *Probabilistic Graphical Models: Principles and Techniques* by Koller and Friedman is extremely detailed. I used aspects of this text to supplement the readings from Jordan's textbook, when I needed additional detail to understand the content of the lecture slides, particularly on representation. My only reservations about this text however is that it can be overwhelming to those who are relatively new to graphical models, and some of the notation can feel unconventional (many of the course slides adapt material and therefore notation from Jordan's textbook).

Due to the accelerated pace at which the course presents foundational topics, if one is not careful, in my view, it is possible to get 'lost in the details' of Koller and Friedman's comprehensiveness, and lose sight of the big picture. However, I have no doubt that their textbook remains a solid reference for researchers.

Finally, I found *Machine Learning: A Probabilistic Perspective* by Murphy also to be an excellent all round reference textbook for both the foundational and more esoteric topics covered in this course. While I'm not sure it is a book that could be exclusively used for self-study, I found this book to be excellent as a go-to reference for more conventional machine learning techniques.

In particular, where Murphy's book excelled for me was that it contains many sections on more modern techniques in machine learning papers which have become assimilated into the 'canon' of required knowledge e.g. latent Dirichlet allocation, conditional random fields etc. Furthermore, I found myself using this as a precursor reference for approximate inference techniques in the course, such as variational inference, Markov chain Monte Carlo methods; and for topics such as loopy belief propagation and the theory of variational inference. This was useful because the papers which discuss those topics can get theoretically demanding quite quickly e.g. the entire 308 page Wainwright and Jordan (2008) paper, of which chapters 3 and 4 are prescribed as reading in lecture 12, is described in Murphy as the 'monster' for this reason.

In terms of the course textbooks that I bought hard-copies of, I already had a copy of *Machine Learning: A Probabilistic Perspective*. The draft textbook *Introduction to Probabilistic Graphical Models* is freely available online, and is also available to download in its entirety from [this subdirectory][GR5] of my GitHub repo. The sample chapters of *Probabilistic Graphical Models: Principles and Techniques* are sufficient to get by in the course. However, I did later purchase a copy of this textbook as a general reference.

## **Lecture summaries, session notes, review notes.**

As part of self-study and to assist in my understanding, I kept a bullet-pointed high-level summary of the content of each recorded lecture in a single PDF document. This document is available below:

[Lecture summaries][LS].

The recorded video lectures contains valuable information on the motivation behind some of the techniques introduced, as well as further exposition and annotations of selected parts of the lecture slides. In order to aid my understanding, I made handwritten notes on the points of emphasis communicated orally by Eric Xing. These notes will also contain core details on models or techniques copied from the slides, and while this may be repetitious, it aided my understanding, and gave me pointers on what I needed to pick up during review. They are listed as *session notes* below.

Some parts of the lecture slides are given less emphasis in the recordings, with the expectation that they will be reviewed in one's own time. It was also the case that some of the slides were too technical to be able to fluently assimilate during the lecture, or queries emerged that required review and supplementary reading. On top of that, there are also fairly technical paper readings that need to be completed. These notes are listed as *review notes* below.

Lecture 1 - Introduction to GMs. [[Session notes][L1SN]. \| [Review notes][L1RN].]

Lecture 2 - Representation: Directed GMs (BNs). [[Session notes][L2SN]. \| [Review notes][L2RN].]

Lecture 3 - Representation: Undirected GMs (MRFs). [[Session notes][L3SN]. \| [Review notes][L3RN].]

Lecture 4 - Exact inference. [[Session notes][L4SN]. \| [Review notes][L4RN].]

Lecture 5 - Parameter learning in fully observable BNs. [[Session notes][L5SN]. \| [Review notes][L5RN].]

Lecture 6 - Parameter learning of partially observed BNs. [[Session notes][L6SN]. \| [Review notes][L6RN].]

Lecture 7 - Maximum likelihood learning of undirected GMs. [[Session notes][L7SN]. \| [Review notes][L7RN].]

Lecture 8 - Causal discovery and inference. [[Session notes][L8SN]. ]

Lecture 9 - Modelling networks. [[Session notes][L9SN]. \| [Review notes][L9RN].]

Lecture 10 - Sequential models. [[Session notes][L10SN]. \| [Review notes][L10RN].]

Lecture 11 - Approximate inference: Mean-field (MF) and loopy belief propagation (BP) approximations. [[Session notes][L11SN]. \| [Review notes][L11RN].]

Lecture 12 - Theory of variational inference: Inner and outer approximations. [[Session notes][L12SN]. \| [Review notes][L12RN].]

Lecture 13 - Approximate inference: Monte Carlo and sequential Monte Carlo methods. [[Session notes][L13SN]. \| [Review notes][L13RN].]

Lecture 14 - Markov chain Monte Carlo. [[Session notes][L14SN]. \| Review notes.]

Lecture 15 - Statistical and algorithmic foundations of deep learning. [[Session notes][L15SN]. \| Review notes.]

Lecture 16 - Building blocks of deep learning (DL). 

Lecture 17 - Deep generative models (part 1). 

Lecture 18 - Deep generative models (part 2). 

Lecture 19 - Case study: Text generation. 

Lecture 20 - Sequential decision making (part 1). 

Lecture 21 - Sequential decision making (part 2). 

Lecture 22 - Bayesian nonparametrics (part 1). 

Lecture 23 - Bayesian nonparametrics (part 2). 

Lecture 24 - Integrative paradigms of GMs: Regularized Bayesian methods. 

Lecture 25 - Elements of spectral and kernel GMs.

Lecture 26 - Gaussian processes (GPs) and elements of meta-learning. 

Lecture 27 - Scalable algorithms and systems. 

Lecture 28 - A civil engineering perspective on AI.

All scanned notes can be downloaded in their entirety from [this subdirectory][GR6] of my GitHub repo.

*Pending complete upload of notes.*

## **Homework assignments: my write-ups.**

My write-ups of each homework assignment are listed below.

* Homework 1 write-up.
* Homework 2 write-up.
* Homework 3 write-up.
* Homework 4 write-up.

*To insert links after upload.*

For questions involving the hand-coded implementations of algorithms, the accompanying code repositories are listed below.

*To insert links after upload.*

## **Supplementary texts.**

The following are supplementary reference textbooks not already listed that I found useful for introductory reading, or for remedying gaps in my knowledge, or as references to supplement the course materials.

As previously mentioned under [*"Additional comments on course prerequisites"*](#additional-comments-on-course-prerequisites), listed below is a link to *"Chapter 8: Graphical Models"* of *Pattern Recognition and Machine Learning* by Bishop (2006). I would recommend reading the introduction, the preliminary part of section 8.1 on *Bayesian networks* up to but not including the first example on *polynomial regression* in section 8.1.1; all of the parts of section 8.2 on *conditional independence* up to and including the definition and example of *d-separation* in 8.2.2; and all the parts of section 8.3 on *Markov random fields* up to but not including section 8.3.2 on *factorization properties*.

[Chapter 8: Graphical models, from Bishop, C. (2006). *Pattern Recognition and Machine Learning.* Springer-Verlag New York Inc.](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/05/Bishop-PRML-sample.pdf)

As previously mentioned under [*"Additional comments on course prerequisites"*](#additional-comments-on-course-prerequisites), familiarity with the fundamentals of Bayesian statistics is *essential*.

In spite of the fact that probabilistic graphical models are inherently neither frequentist nor Bayesian tools, they are most often encountered in Bayesian settings. A basic literacy with Bayesian statistics is therefore required to understand both the foundational topics, as well as the more modern methods and models in the course, such as latent Dirichlet allocation, variational inference, Markov chain Monte Carlo methods, Gaussian processes, and Bayesian nonparametrics. In particular, some of the assignments require developing familiarity with *hierarchical Bayesian models*. For these reasons, I both used and would recommend the well-known reference textbook below on Bayesian statistics, which also has some sections on the more modern topics covered in the course:

Gelman, A., Carlin, J., Stern, H., Dunson., D., Vehtari, A., Rubin, D. (2013). *Bayesian Data Analysis (3rd ed.).* Chapman and Hall.

<p align="center">
  <img src="/assets/2021-06-08-self-study-10-708/BDA.jpg">
</p>

There are many excellent introductory tutorial papers written on the class of *simulation-based methods*, to which sampling, Monte Carlo and Markov chain Monte Carlo methods belong. However, given that Markov chain Monte Carlo methods arguably revolutionised Bayesian statistics, I wanted an authoritative reference textbook on these methods to supplement the introductory presentation from the course. For an in-depth graduate-level perspective into the theory of all the above methods, I used and would recommend the following:

Robert, C., Casella, G. (2010). *Monte Carlo Statistical Methods (2nd ed.).* Springer-Verlag New York Inc.

<p align="center">
  <img src="/assets/2021-06-08-self-study-10-708/MCSM.jpg">
</p>

## **Electronic copies of reference textbooks.**

Whilst in an ideal world one would have hard copies of all the books here, I recognise that there are many who are financially constrained and not able to purchase the books for themselves.

Almost all of the reference textbooks, supplementary textbooks and textbooks referenced in the readings are freely available online, often kindly by choice of the authors themselves.

[Jordan, M. (draft). *An Introduction to Probabilistic Graphical Models.*](https://people.eecs.berkeley.edu/~jordan/prelims/)

[MacKay, D. (2003). *Information Theory, Inference and Learning Algorithms.* Cambridge University Press.](https://www.inference.org.uk/itprnn/book.pdf)

[Goodfellow, I., Bengio, Y. Courville, A. (2016). *Deep Learning.* MIT Press.](https://www.deeplearningbook.org/)

[Sutton, R., Barto A. (2018). *Reinforcement Learning: An Introduction. (2nd ed.)*.  MIT Press.](http://incompleteideas.net/book/RLbook2020.pdf)

[Rasmussen, C., Williams, C. (2006). *Gaussian Processes for Machine Learning.* MIT Press.](http://www.gaussianprocess.org/gpml/chapters/RW.pdf)

[Gelman, A., Carlin, J., Stern, H., Dunson., D., Vehtari, A., Rubin, D. (2013). *Bayesian Data Analysis (3rd ed.).* Chapman and Hall.](http://www.stat.columbia.edu/~gelman/book/BDA3.pdf)

[Robert, C., Casella, G. (2010). *Monte Carlo Statistcal Methods (2nd ed.).* Springer-Verlag New York Inc.](https://mcube.nctu.edu.tw/~cfung/docs/books/robert2004monte_carlo_statistical_methods.pdf)

At the time of writing, these links were all working. I will not actively maintain these links. Furthermore, some of the links above are not the most up-to-date editions, which may or may not be appropriate for one’s purposes.

In the event that a link goes dead, and for the reference textbooks that are not included in the list above, if one really needs access to an electronic copy, the following is a possible solution:

* Visit the Genesis Library, a link to which is contained in the following [Wikipedia article](https://en.wikipedia.org/wiki/Library_Genesis).

[L1a]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/lecture-slides/lecture01-introduction-14-jan.pdf
[L1b]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/lecture-slides-annotated/lecture01-introduction-14-jan-annotated.pdf
[L1c]: https://www.youtube.com/watch?v=Spj2OSYmtGg&list=PLoZgVqqHOumTY2CAQHL45tQp6kmDnDcqn&index=1
[L1d]: https://sailinglab.github.io/pgm-spring-2019/notes/lecture-01/
[L1r1]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/readings/lecture01-readings/jordan-graphical-models-2004.pdf
[L1r2]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/readings/lecture01-readings/airoldi-getting-started-2007.pdf

[L2a]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/lecture-slides/lecture02-bn-representation-16-jan.pdf
[L2b]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/lecture-slides-annotated/lecture02-bn-representation-16-jan-annotated.pdf
[L2c]: https://www.youtube.com/watch?v=4ZwNLLXZenk&list=PLoZgVqqHOumTY2CAQHL45tQp6kmDnDcqn&index=2
[L2d]: https://sailinglab.github.io/pgm-spring-2019/notes/lecture-02/
[L2r1]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/readings/lecture02-readings/ch2-jordan.pdf
[L2r2]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/readings/lecture02-readings/ch3-koller-friedman.pdf
 
[L3a]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/lecture-slides/lecture03-mrf-representation-23-jan.pdf
[L3b]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/lecture-slides-annotated/lecture03-mrf-representatio-annotated.pdf
[L3c]: https://www.youtube.com/watch?v=MFqRIgzi9vQ&list=PLoZgVqqHOumTY2CAQHL45tQp6kmDnDcqn&index=3
[L3d]: https://sailinglab.github.io/pgm-spring-2019/notes/lecture-03/
[L3r1]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/readings/lecture03-readings/ch2-jordan.pdf
[L3r2]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/readings/lecture03-readings/ch4-koller-friedman.pdf
[L3r3]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/readings/lecture03-readings/fischer-igel-introduction-restricted-2012.pdf


[L4a]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/lecture-slides/lecture04-exact-inference-28-jan.pdf
[L4b]:  https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/lecture-slides-annotated/lecture04-exact-inference-28-jan-annotated.pdf
[L4c]: https://www.youtube.com/watch?v=LBMNMWgp8PU&list=PLoZgVqqHOumTY2CAQHL45tQp6kmDnDcqn&index=4
[L4d]: https://sailinglab.github.io/pgm-spring-2019/notes/lecture-04/
[L4r1]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/readings/lecture04-readings/ch3-jordan.pdf
[L4r2]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/readings/lecture04-readings/ch4-jordan.pdf
[L4r3]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/readings/lecture04-readings/ch-9.1-9.4-koller-friedman.pdf
[L4r4]:https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/readings/lecture04-readings/ch-10.1-10.3-koller-friedman.pdf
[L4r5]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/readings/lecture04-readings/minka-divergence-measures-2005.pdf

[L5a]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/lecture-slides/lecture05-parameter-learning30-jan.pdf
[L5d]: https://sailinglab.github.io/pgm-spring-2019/notes/lecture-05/
[L5r1]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/readings/lecture05-readings/ch8-jordan.pdf
[L5r2]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/readings/lecture05-readings/ch9-jordan.pdf
[L5r3]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/readings/lecture05-readings/ch-17.1-17.4-koller-friedman.pdf
[L5r4]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/readings/lecture05-readings/geiger-heckerman-characterization-dirichlet-1997.pdf
[L5r5]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/readings/lecture05-readings/geiger-heckerman-parameter-priors-2002.pdf

[L6a]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/lecture-slides/lecture06-em-algorithm-4-feb.pdf
[L6b]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/lecture-slides-annotated/lecture06-em-algorithm-4-feb-annotated.pdf
[L6c]: https://www.youtube.com/watch?v=XjikIgzLT9E&list=PLoZgVqqHOumTY2CAQHL45tQp6kmDnDcqn&index=5
[L6d]: https://sailinglab.github.io/pgm-spring-2019/notes/lecture-06/
[L6r1]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/readings/lecture06-readings/ch11-jordan.pdf
[L6r2]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/readings/lecture06-readings/ch-19.1-19.4-koller-friedman.pdf
[L6r3]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/readings/lecture06-readings/borman-expectation-maximization-2006.pdf
[L6r4]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/readings/lecture06-readings/neal-hinton-view-em-1998.pdf


[L7a]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/lecture-slides/lecture07-mrf-maximum-likelihood-6-feb.pdf
[L7b]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/lecture-slides-annotated/lecture07-mrf-maximum-likelihood-6-feb-annotated.pdf
[L7c]: https://www.youtube.com/watch?v=BUmEdFYkdRw&list=PLoZgVqqHOumTY2CAQHL45tQp6kmDnDcqn&index=6
[L7d]: https://sailinglab.github.io/pgm-spring-2019/notes/lecture-07/
[L7r1]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/readings/lecture07-readings/ch9-jordan.pdf
[L7r2]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/readings/lecture07-readings/ch20-jordan.pdf
[L7r3]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/readings/lecture07-readings/friedman-ea-sparse-inverse-2007.pdf
[L7r4]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/readings/lecture07-readings/lafferty-ea-conditional-random-2001.pdf


[L8a]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/lecture-slides/lecture08-causality-11-feb.pdf
[L8c]: https://www.youtube.com/watch?v=jIziEg0NoTo&list=PLoZgVqqHOumTY2CAQHL45tQp6kmDnDcqn&index=7
[L8d]: https://sailinglab.github.io/pgm-spring-2019/notes/lecture-08/
[L8r1]: http://bayes.cs.ucla.edu/PRIMER/
[L8r2]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/readings/lecture08-readings/causation-prediction-search-book-spirtes-scheines.pdf
[L8r3]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/readings/lecture08-readings/zhang-ea-learning-causality-2017.pdf

[L9a]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/lecture-slides/lecture09-modelling-networks-13-feb.pdf
[L9b]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/lecture-slides-annotated/lecture09-modelling-networks-annotated.pdf
[L9c]: https://www.youtube.com/watch?v=VM4Nd7O2UWY&list=PLoZgVqqHOumTY2CAQHL45tQp6kmDnDcqn&index=8
[L9d]: https://sailinglab.github.io/pgm-spring-2019/notes/lecture-09/
[L9r1]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/readings/lecture09-readings/meinshausen-buhlmann-highdimensional-graphs-2006.pdf
[L9r2]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/readings/lecture09-readings/kolar-ea-estimating-time-2010.pdf
[L9r3]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/readings/lecture09-readings/dempster-covariance-selection-1972.pdf

[L10a]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/lecture-slides/lecture10-sequentialmodels-18-feb.pdf
[L10b]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/lecture-slides-annotated/lecture10-sequentialmodels-18-feb-annotated.pdf
[L10c]: https://www.youtube.com/watch?v=ko4zGBEUCok&list=PLoZgVqqHOumTY2CAQHL45tQp6kmDnDcqn&index=9
[L10d]: https://sailinglab.github.io/pgm-spring-2019/notes/lecture-10/
[L10r1]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/readings/lecture10-readings/ch14-jordan.pdf
[L10r2]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/readings/lecture10-readings/ch15-jordan.pdf
[L10r3]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/readings/lecture10-readings/factor-analysys-cs229-ng.pdf
[L10r4]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/readings/lecture10-readings/welch-bishop-introduction-kalman-2006.pdf
[L10r5]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/readings/lecture10-readings/wallach-conditional-random-2004.pdf

[L11a]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/lecture-slides/lecture11-mf-bp-approximations-20-feb.pdf
[L11b]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/lecture-slides-annotated/lecture11-mf-bp-approximations-20-feb-annotated.pdf
[L11c]: https://www.youtube.com/watch?v=cBOgC8WFLMQ&list=PLoZgVqqHOumTY2CAQHL45tQp6kmDnDcqn&index=10
[L11d]: https://sailinglab.github.io/pgm-spring-2019/notes/lecture-11/
[L11r1]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/readings/lecture11-readings/yedidia-ea-generalized-belief-2000.pdf
[L11r2]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/readings/lecture11-readings/xing-ea-generalized-mean-2012.pdf
[L11r3]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/readings/lecture11-readings/mohamed-ea-variational-inference-2016.pdf


[L12a]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/lecture-slides/lecture12-variational-inference-25-feb.pdf
[L12b]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/lecture-slides-annotated/lecture12-variational-inference-25-feb-annotated.pdf
[L12c]: https://www.youtube.com/watch?v=r8EeHbtUk_E&list=PLoZgVqqHOumTY2CAQHL45tQp6kmDnDcqn&index=11
[L12d]: https://sailinglab.github.io/pgm-spring-2019/notes/lecture-12/
[L12r1]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/readings/lecture12-readings/wainwright-jordan-variational-inference-2003.pdf
[L12r2]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/readings/lecture12-readings/wainwright-jordan-graphical-models-2008.pdf


[L13a]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/lecture-slides/lecture13-monte-carlo-27-feb.pdf 
[L13b]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/lecture-slides-annotated/lecture13-monte-carlo-27-feb-annotated.pdf
[L13c]: https://www.youtube.com/watch?v=aexfhQwcTO0&list=PLoZgVqqHOumTY2CAQHL45tQp6kmDnDcqn&index=12
[L13d]: https://sailinglab.github.io/pgm-spring-2019/notes/lecture-13/
[L13r1]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/readings/lecture13-readings/ch21-jordan.pdf
[L13r2]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/readings/lecture13-readings/ch-29-mackay.pdf

[L14a]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/lecture-slides/lecture14-mcmc-4-mar.pdf
[L14b]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/lecture-slides-annotated/lecture14-mcmc-4-mar-annotated.pdf
[L14c]: https://www.youtube.com/watch?v=oCRqPe-MlYc&list=PLoZgVqqHOumTY2CAQHL45tQp6kmDnDcqn&index=13
[L14d]: https://sailinglab.github.io/pgm-spring-2019/notes/lecture-14/
[L14r1]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/readings/lecture14-readings/ch-29-mackay.pdf
[L14r2]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/readings/lecture14-readings/neal-mcmc-hamiltonian-2012.pdf
[L14r3]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/readings/lecture14-readings/patterson-teh-stochastic-gradient-2013.pdf
 
[L15a]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/lecture-slides/lecture15-deep-learning-foundations-6-mar.pdf
[L15b]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/lecture-slides-annotated/lecture15-deep-learning-foundations-6-mar-annotated.pdf
[L15c]: https://www.youtube.com/watch?v=6vRnbht-L5M&list=PLoZgVqqHOumTY2CAQHL45tQp6kmDnDcqn&index=14
[L15d]: https://sailinglab.github.io/pgm-spring-2019/notes/lecture-15/
[L15r1]: https://www.deeplearningbook.org/contents/mlp.html
[L15r2]: https://www.deeplearningbook.org/contents/generative_models.html
[L15r3]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/readings/lecture15-readings/salakhutdinov-hinton-deep-boltzmann-2009.pdf
[L15r4]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/readings/lecture15-readings/belanger-mccallum-structured-prediction-2016.pdf
[L15r5]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/readings/lecture15-readings/ranganath-ea-deep-exponential-2015.pdf


[L16a]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/lecture-slides/lecture16-dl-blocks-zhiting-18-mar.pdf
[L16c]: https://www.youtube.com/watch?v=Mgzwmlf5ap8&list=PLoZgVqqHOumTY2CAQHL45tQp6kmDnDcqn&index=15
[L16d]: https://sailinglab.github.io/pgm-spring-2019/notes/lecture-16/
[L16r1]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/readings/lecture16-readings/pascanu-ea-difficulty-training-2013.pdf
[L16r2]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/readings/lecture16-readings/vaswani-ea-attention-2017.pdf
[L16r3]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/readings/lecture16-readings/devlin-ea-bert-2019.pdf


[L17a]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/lecture-slides/lecture17-dgm-pt1-20-mar.pdf
[L17c]: https://www.youtube.com/watch?v=hkiRiY6RPHo&list=PLoZgVqqHOumTY2CAQHL45tQp6kmDnDcqn&index=16
[L17d]: https://sailinglab.github.io/pgm-spring-2019/notes/lecture-17/
[L17r1]: https://www.deeplearningbook.org/contents/generative_models.html
[L17r2]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/readings/lecture17-readings/kingma-welling-autoencoding-variational-2014.pdf
[L17r3]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/readings/lecture17-readings/goodfellow-ea-generative-adversarial-2014.pdf
[L17r4]:http://www.offconvex.org/2017/03/15/GANs/

[L18a]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/lecture-slides/lecture18-dgm-pt2-zhiting-25-mar.pdf
[L18c]: https://www.youtube.com/watch?v=lHX29lvt_0M&list=PLoZgVqqHOumTY2CAQHL45tQp6kmDnDcqn&index=17
[L18d]: https://sailinglab.github.io/pgm-spring-2019/notes/lecture-18/
[L18r1]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/readings/lecture18-readings/arjovsky-bottou-principled-methods-2017.pdf
[L18r2]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/readings/lecture18-readings/kingma-dhariwal-glow-2018.pdf
[L18r3]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/readings/lecture18-readings/hu-ea-harnessing-deep-2020.pdf
[L18r4]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/readings/lecture18-readings/hu-ea-deep-generative-2018.pdf

[L19a]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/lecture-slides/lecture19-text-gen-zhiting-27-mar.pdf  
[L19c]: https://www.youtube.com/watch?v=c3vRNaC93mw&list=PLoZgVqqHOumTY2CAQHL45tQp6kmDnDcqn&index=18
[L19d]: https://sailinglab.github.io/pgm-spring-2019/notes/lecture-19/
[L19r1]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/readings/lecture19-readings/ranzato-ea-sequence-level-2016.pdf
[L19r2]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/readings/lecture19-readings/hu-ea-controlled-generation-2018.pdf
[L19r3]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/readings/lecture19-readings/tan-ea-connecting-dots-2019.pdf

[L20a]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/lecture-slides/lecture20-rl%2Bpgm-pt1-maruan-01-apr.pdf
[L20b]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/lecture-slides-annotated/lecture20-rl%2Bpgm-pt1-maruan-01-apr-annotated.pdf
[L20c]: https://www.youtube.com/watch?v=PklKJKh-IWY&list=PLoZgVqqHOumTY2CAQHL45tQp6kmDnDcqn&index=19
[L20d]: https://sailinglab.github.io/pgm-spring-2019/notes/lecture-20/
[L20r1]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/readings/lecture20-readings/ch3-sutton-barto.pdf
[L20r2]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/readings/lecture20-readings/ch4-sutton-barto.pdf
[L20r3]: https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html
[L20r4]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/readings/lecture20-readings/levine-reinforcement-learning-2018.pdf
[L20r5]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/readings/lecture20-readings/ziebart-modeling-purposeful-2010.pdf
[L20r6]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/readings/lecture20-readings/todorov-general-duality-2008.pdf
[L20r7]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/readings/lecture20-readings/ch20-koller-friedman.pdf

[L21a]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/lecture-slides/lecture21-rl%2Bpgm-pt2-maruan-03-apr-annotated.pdf
[L21b]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/lecture-slides-annotated/lecture21-rl%2Bpgm-pt2-maruan-03-apr-annotated.pdf
[L21c]: https://www.youtube.com/watch?v=9KD65Ms-KHk&list=PLoZgVqqHOumTY2CAQHL45tQp6kmDnDcqn&index=20
[L21d]: https://sailinglab.github.io/pgm-spring-2019/notes/lecture-21/
[L21r1]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/readings/lecture21-readings/ch13-sutton-barto.pdf
[L21r2]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/readings/lecture21-readings/levine-reinforcement-learning-2018.pdf
[L21r3]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/readings/lecture21-readings/haarnoja-ea-reinforcement-learning-2017.pdf
[L21r4]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/readings/lecture21-readings/haarnoja-ea-soft-actor-2018.pdf

[L22a]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/lecture-slides/lecture22-bayesian-non-parametrics-pt1-08-apr.pdf
[L22b]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/lecture-slides-annotated/lecture22-bayesian-non-parametrics-pt1-08-apr-annotated.pdf
[L22c]: https://www.youtube.com/watch?v=bCb2xvIuMOs&list=PLoZgVqqHOumTY2CAQHL45tQp6kmDnDcqn&index=21
[L22d]: https://sailinglab.github.io/pgm-spring-2019/notes/lecture-22/
[L22r1]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/readings/lecture22-readings/teh-dirichlet-process.pdf
[L22r2]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/readings/lecture22-readings/griffiths-gharamani-indian-buffet-2011.pdf

[L23a]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/lecture-slides/lecture23-bayesian-non-parametrics-pt2-10-apr.pdf
[L23c]: https://www.youtube.com/watch?v=pqgbWOh_g3Q&list=PLoZgVqqHOumTY2CAQHL45tQp6kmDnDcqn&index=22
[L23d]: https://sailinglab.github.io/pgm-spring-2019/notes/lecture-23/
[L23r1]: https://dp.tdhopper.com/
[L23r2]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/readings/lecture23-readings/ishwaran-gibbs-sampling-2001.pdf
[L23r3]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/readings/lecture23-readings/teh-ea-hierarchical-dirichlet-2005.pdf

[L24a]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/lecture-slides/lecture24-integrative-paradigms-15-apr.pdf
[L24b]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/lecture-slides-annotated/lecture24-integrative-paradigms-15-apr-annotated.pdf
[L24c]: https://www.youtube.com/watch?v=5HxU_01vOc4&list=PLoZgVqqHOumTY2CAQHL45tQp6kmDnDcqn&index=23
[L24d]: https://sailinglab.github.io/pgm-spring-2019/notes/lecture-24/
[L24r1]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/readings/lecture24-readings/ganchev-ea-posterior-regularization-2010.pdf
[L24r2]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/readings/lecture24-readings/zhu-ea-bayesian-inference-2014.pdf

[L25a]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/lecture-slides/lecture25-spectral-kernel-gms-17-apr.pdf
[L25b]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/lecture-slides-annotated/lecture22-bayesian-non-parametrics-pt1-08-apr-annotated.pdf
[L25c]: https://www.youtube.com/watch?v=zTa36P2gVSA&list=PLoZgVqqHOumTY2CAQHL45tQp6kmDnDcqn&index=24
[L25d]: https://sailinglab.github.io/pgm-spring-2019/notes/lecture-25/
[L25r1]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/readings/lecture25-readings/song-learning-hilbert-2008.pdf
[L25r2]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/readings/lecture25-readings/hsu-ea-spectral-algorithm-2012.pdf
[L25r3]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/readings/lecture25-readings/song-ea-nonparametric-latent-2014.pdf
[L25r4]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/readings/lecture25-readings/parikh-ea-spectral-algorithm-latent-2014.pdf

[L26a]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/lecture-slides/lecture26-gp-maruan-22-apr.pdf
[L26c]: https://www.youtube.com/watch?v=X3qt3ZKw8vw&list=PLoZgVqqHOumTY2CAQHL45tQp6kmDnDcqn&index=25
[L26d]: https://sailinglab.github.io/pgm-spring-2019/notes/lecture/
[L26r1]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/readings/lecture26-readings/ch2-rasmussen-williams.pdf
[L26r2]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/readings/lecture26-readings/ch4-rasmussen-williams.pdf
[L26r3]: https://distill.pub/2019/visual-exploration-gaussian-processes/
[L26r4]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/readings/lecture26-readings/wilson-ea-deep-kernel-2015.pdf
[L26r5]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/readings/lecture26-readings/garnelo-ea-neural-processes-2018.pdf

[L27a]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/lecture-slides/lecture27-scalable-algorithms-and-systems-qirong-24-apr.pdf
[L27c]: https://www.youtube.com/watch?v=-Sah_kO458o&list=PLoZgVqqHOumTY2CAQHL45tQp6kmDnDcqn&index=26
[L27d]: https://sailinglab.github.io/pgm-spring-2019/notes/lecture-27/
[L27r1]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/readings/lecture27-readings/bradley-ea-parallel-coordinate-2016.pdf
[L27r2]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/readings/lecture27-readings/yuan-ea-lightlda-2015.pdf
[L27r3]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/readings/lecture27-readings/ho-ea-effective-distributed-2013.pdf
[L27r4]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/readings/lecture27-readings/kim-ea-strads-2016.pdf

[L28a]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/lecture-slides/lecture28-ai-civil-engineering-29-apr.pdf
[L28b]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/lecture-slides-annotated/lecture28-ai-civil-engineering-29-apr-annotated.pdf
[L28c]: https://www.youtube.com/watch?v=JCV8jRYcCgg&list=PLoZgVqqHOumTY2CAQHL45tQp6kmDnDcqn&index=27
[L28d]: https://sailinglab.github.io/pgm-spring-2019/notes/lecture-28/
[L28r1]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/readings/lecture28-readings/xing-ea-petuum-2015.pdf
[L28r2]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/readings/lecture28-readings/xing-ea-strategies-principles-2016.pdf

[HW1a]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/assignments/hw-1/hw-1-v1.1.pdf
[HW1b]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/assignments/hw-1/hw-1-latex_v1.1.zip
[HW1c]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/assignments/hw-1/hw-1-code.zip
[HW1d]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/assignments/hw-1/hmm_note.pdf

[HW2a]:https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/assignments/hw-2/hw-2-v1.1.pdf
[HW2b]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/assignments/hw-2/hw-2-latex.zip
[HW2c]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/assignments/hw-2/hw-2-data.zip

[HW3a]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/assignments/hw-3/hw-3.pdf
[HW3b]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/assignments/hw-3/hw-3-latex.zip
[HW3c]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/assignments/hw-3/hw-3-tutorial-wake-sleep-vae.ipynb

[HW4a]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/assignments/hw-4/hw-4.pdf
[HW4b]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/assignments/hw-4/hw-4-latex.zip
[HW4c]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/assignments/hw-4/hw_4-tutorial-rl-probabilistic-inference.ipynb

[LS]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/10-708-lecture-summaries.pdf

[L1SN]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/self-study-scanned-notes/lecture-01-introduction-session-notes.pdf
[L1RN]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/self-study-scanned-notes/lecture-01-review-notes.pdf

[L2SN]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/self-study-scanned-notes/lecture-02-bn-representation-session-notes.pdf
[L2RN]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/self-study-scanned-notes/lecture-02-review-notes.pdf

[L3SN]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/self-study-scanned-notes/lecture-03-mrf-representation-session-notes.pdf
[L3RN]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/self-study-scanned-notes/lecture-03-review-notes.pdf

[L4SN]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/self-study-scanned-notes/lecture-04-exact-inference-session-notes.pdf
[L4RN]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/self-study-scanned-notes/lecture-04-review-notes.pdf

[L5SN]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/self-study-scanned-notes/lecture-05-parameter-learning-session-notes.pdf
[L5RN]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/self-study-scanned-notes/lecture-05-review-notes.pdf

[L6SN]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/self-study-scanned-notes/lecture-06-em-algorithm-session-notes.pdf
[L6RN]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/self-study-scanned-notes/lecture-06-review-notes.pdf

[L7SN]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/self-study-scanned-notes/lecture-07-mrf-maximum-likelihood-session-notes.pdf
[L7RN]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/self-study-scanned-notes/lecture-07-review-notes.pdf

[L8SN]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/self-study-scanned-notes/lecture-08-causality-session-notes.ipynb

[L9SN]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/self-study-scanned-notes/lecture-09-modelling-networks-session-notes.pdf
[L9RN]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/self-study-scanned-notes/lecture-09-review-notes.pdf

[L10SN]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/self-study-scanned-notes/lecture-10-sequential-models-session-notes.pdf
[L10RN]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/self-study-scanned-notes/lecture-10-review-notes.pdf

[L11SN]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/self-study-scanned-notes/lecture-11-mf-bp-approximations-session-notes.pdf
[L11RN]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/self-study-scanned-notes/lecture-11-review-notes.pdf

[L12SN]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/self-study-scanned-notes/lecture-12-variational-inference-session-notes.pdf
[L12RN]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/self-study-scanned-notes/lecture-12-review-notes.pdf

[L13SN]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/self-study-scanned-notes/lecture-13-monte-carlo-session-notes.pdf
[L13RN]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/self-study-scanned-notes/lecture-13-review-notes.pdf

[L14SN]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/self-study-scanned-notes/lecture-14-mcmc-session-notes.pdf

[L15SN]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/blob/master/self-study-scanned-notes/lecture-15-deep-learning-foundations-session-notes.pdf

[GR1]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/tree/master/lecture-slides
[GR2]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/tree/master/lecture-slides-annotated
[GR3]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/tree/master/readings
[GR4]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/tree/master/assignments
[GR5]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/tree/master/readings/intro-to-pgms-book-jordan
[GR6]: https://github.com/cyber-rhythms/cmu-10-708-probabilistic-graphical-models-spring-2019/tree/master/self-study-scanned-notes
 